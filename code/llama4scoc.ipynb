{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ed690-a89b-42c7-b9b9-339ee382c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# program for summarize several pdfs about SCOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3640bbda-7ade-4770-b359-803ee063eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /tmp/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3046.88 MiB\n",
      "llama_new_context_with_model: KV self size  = 3046.88 MiB, K (f16): 1523.44 MiB, V (f16): 1523.44 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   342.31 MiB\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "/home/mardom/anaconda3/envs/transformers/lib/python3.9/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=\"local\", embed_model='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d642d776-3b46-4f14-8457-455335598b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, llm=\"local\", embed_model='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f099bf9-decc-4d80-948d-ee892427ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe902d-5548-475d-b3b6-183d12d54f44",
   "metadata": {},
   "source": [
    "#### (Optional) Load the Index if already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd9f9d1-2b5f-4ce5-b944-c546b109a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import load_index_from_storage, StorageContext\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89e155-db4e-496c-8816-9ba1a6690fe6",
   "metadata": {},
   "source": [
    "### Try Asking a question\n",
    "\n",
    "Due to processing constraints, setting `similarity_top_k=1` is an ideal setting. Otherwise, responses will be quite slow due to the speed of CPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29570baa-e6b8-4c52-b5a1-610bf9c29c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8b81a-dd58-463f-a069-d04e2784b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What are the main SCs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a2b9f-deee-4e88-a908-bd4b4e5beca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is one of the great achievements from the Kepler mission?\")\n",
    "print(response)                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f393695-395a-484a-8364-bbf9deccc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is requirements for observing variable stars with Rubin Observatory?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175d1be-f750-48bf-b500-d796821769d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is requirements for observing RR Lyrae stars in the galactic halo with Rubin Observatory?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04621e17-026b-47bf-8ce8-8f0d1e59b4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   72629.51 ms\n",
      "llama_print_timings:      sample time =       8.93 ms /    41 runs   (    0.22 ms per token,  4589.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =  378763.04 ms /  2392 tokens (  158.35 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:        eval time =    8438.78 ms /    40 runs   (  210.97 ms per token,     4.74 tokens per second)\n",
      "llama_print_timings:       total time =  387295.95 ms /  2432 tokens\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m esponse \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich is the area covered by the WFD survey of the Rubin Observatory?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Which is the area covered by the WFD survey of the Rubin Observatory?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58d0935e-0a41-449d-bd96-430531c315e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hello! I'm here to help you with your question. A rolling cadence is an observing strategy used in astronomy, specifically for the Large Synoptic Survey Telescope (LSST) survey. It involves dividing the sky into different regions and observing each region at a higher cadence than the rest. This means that some regions of the sky are observed more frequently than others.\n",
      "\n",
      "The rolling cadence is useful for transient science, as it allows for better sampling over a smaller period of time, which can lead to more detections of transient events such as supernovae and gamma-ray bursts. Additionally, the light-curve quality scores are improved with the rolling cadence, except for the alt_dust strategy, which observes either a southern or northern region of the sky each night.\n",
      "\n",
      "The trade-off with the rolling cadence is that it reduces the number of detected transients, but the light-curve quality is improved. The number of detected transients scales with the number of observations, so including a rolling cadence in the observing strategy can lead to fewer detections. However, the light-curve quality score is increased, which can be benef\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   72629.51 ms\n",
      "llama_print_timings:      sample time =      57.28 ms /   256 runs   (    0.22 ms per token,  4469.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =  184888.89 ms /  1238 tokens (  149.34 ms per token,     6.70 tokens per second)\n",
      "llama_print_timings:        eval time =   49428.46 ms /   255 runs   (  193.84 ms per token,     5.16 tokens per second)\n",
      "llama_print_timings:       total time =  234831.55 ms /  1493 tokens\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is a rolling cadence and for what is useful?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18e8ed-310a-4056-911b-af78f9c3dd41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
